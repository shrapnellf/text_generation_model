{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_31xspvxr5Of"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "48dJa7udsUSL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the dataset (Shakespeare)"
      ],
      "metadata": {
        "id": "icT6wR4tsnCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    \"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C3R_LBisk1U",
        "outputId": "b2417fa2-c9d4-4d18-81e9-bf3670db6099"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the dataset"
      ],
      "metadata": {
        "id": "6al7OFb7s2NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP9lQ256sxpt",
        "outputId": "36a3186b-f0a7-40e0-ebdb-0326b0613211"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umjJ-jpctOHG",
        "outputId": "b6292c05-9d8a-4a81-9b76-10b090b2b850"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHiMMIoftUGy",
        "outputId": "3c8c871f-96e4-4cfa-c7ef-5a6f59f05a67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Then process the text**\n",
        "\n",
        "Vectorizing the text using keras"
      ],
      "metadata": {
        "id": "g8ZvuN1dtjWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = [\"abcdefg\", \"xyz\"]\n",
        "\n",
        "#TODO 1\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE4vaVl1te5j",
        "outputId": "e03b338f-e976-43ca-e8e8-94e571bd1bf0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "WAebqwBvuA1_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLNIK7Yfu9Xe",
        "outputId": "280d081e-c6e5-4e5e-a836-8afb6a033133"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be human readable, turn the vector to human language"
      ],
      "metadata": {
        "id": "22CPJGnbvMHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "a6ouTvl_vAQt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHS7pwHUv0l8",
        "outputId": "9aaaa1e6-0bfc-41e5-c28f-e5cb913d1661"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to join the characters back to strings\n",
        "\n",
        "tf.strings.reduce_join(chars, axis=1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtGF_3HQv8O3",
        "outputId": "01f42167-4655-45c7-a7ee-e29f4ff386a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "QqdK0QnXwMGB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **The next is prediction task**\n",
        "\n",
        "1st, create training examples and targets"
      ],
      "metadata": {
        "id": "M3M4T-OtwcbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO 2\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLX4YXVJwYng",
        "outputId": "241f4640-0d34-41d7-de9f-cad8f01bd451"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "6PeQYXziw06d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42V4y9Cfw_0v",
        "outputId": "cc377330-a47e-4112-a915-0a89168f740f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "example_per_epoch = len(text) // (seq_length + 1)"
      ],
      "metadata": {
        "id": "Y7LGtC96xQzV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then convert these individual characters to sequence of desired size usung batch method."
      ],
      "metadata": {
        "id": "M6P_KizkyDIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AefpyT8RxvQ9",
        "outputId": "20f9e644-3346-4fef-fcdf-475af5aa443a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#take token back to strings\n",
        "\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFRhZRtQyklv",
        "outputId": "f096af86-7942-4f92-ee87-889e269bb6dc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "take sequence as an input, duplicates, and shift it to align the input and label for each timestep"
      ],
      "metadata": {
        "id": "De26q-bGTKbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "vym5AcK_zAvp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"TensorFlow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn6N569vTk9S",
        "outputId": "e9e50ad7-ca85-4bd2-ce94-7032f4fa812d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'F', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'F', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "nBeA4W9_TsQ-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target :\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "USyBNyz7T_OA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c0586b-fb50-4b07-9e0e-27508d8adc6f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target : b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now create training batches**"
      ],
      "metadata": {
        "id": "7UBdl9LaVNtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "cXnumjFjUeLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f5db4e3-5ae7-4de3-b55f-3522551af833"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Then, build the model!**\n",
        "\n",
        "The layer is:\n",
        "1. tf.keras.layers.Embedding, input layer\n",
        "2. tf.keras.layers.GRU: a type of RNN with size\n",
        "3. tf.keras.layers.Dense: output layer"
      ],
      "metadata": {
        "id": "Qx-dlTEtWJ0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO 3\n",
        "\n",
        "#length of vocabulary in chars\n",
        "vocal_size = len(vocab)\n",
        "\n",
        "#embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "#number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "PAM4d6bWWG6A"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below do:\n",
        "\n",
        "\n",
        "*   derive a class from tf.keras.Model\n",
        "*   constructor use to define the layers of model\n",
        "*   define the pass forward using layers defined in constructor\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8EO_Vo5tW6fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__() #self\n",
        "        # TODO - Create an embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # TODO - Create a GRU layer\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # TODO - Finally connect it with a dense layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        # since we are training a text generation model,\n",
        "        # we use the previous state, in training. If there is no state,\n",
        "        # then we initialize the state\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(batch_size=tf.shape(x)[0])\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "HC99Y2kyW3ni"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ],
      "metadata": {
        "id": "1x3ZLuwKZ8eO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try the model**"
      ],
      "metadata": {
        "id": "xGPrUghMb3My"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AXHUtbiyicNh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CHnJTDlagGh",
        "outputId": "abe5ccf3-0fdf-4615-c3f6-1063daba898d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "v80rGOJdcSEw",
        "outputId": "ce2cbc4c-2bab-4b1f-e100-8c646a846462"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"my_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m16,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m64\u001b[0m,      │       \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "│                                      │ \u001b[38;5;34m1024\u001b[0m))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)               │          \u001b[38;5;34m67,650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "│                                      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                      │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "to get actual predictions from model, need to sample the output distribution."
      ],
      "metadata": {
        "id": "-zoTgWhyer-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0], num_samples=1\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "hbqEjSwWeebM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETnUeYxGfROS",
        "outputId": "6eaacf0e-6c1e-4e16-b0e7-806bd4843944"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([17, 60, 19, 49,  7,  7, 65, 17, 63, 55, 57, 46, 10,  2, 34, 20, 46,\n",
              "        9, 65, 12, 64, 65, 12, 23, 45,  8, 12, 44, 59, 35, 34, 36, 53, 65,\n",
              "       34, 34, 39, 10, 15, 47, 27, 34, 57, 21,  5, 62,  0, 63, 26, 14,  2,\n",
              "       32,  7, 16, 58, 56, 34, 30, 63, 50, 54,  9, 31, 45,  7, 21, 26, 11,\n",
              "        8, 39, 31, 27,  6, 33, 64, 11, 42,  1, 35, 51, 64, 20, 51, 37, 27,\n",
              "       34,  0, 14,  0, 16,  2, 58, 59, 49, 35, 60, 25, 36, 53,  6])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#decode it\n",
        "\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PnwhFS2fTcO",
        "outputId": "031f2e87-494c-4600-d541-8e07951ec60b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"t abuses:\\nTherefore use none: let Romeo hence in haste,\\nElse, when he's found, that hour is his last\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"DuFj,,zDxprg3 UGg.z;yz;Jf-;etVUWnzUUZ3BhNUrH&w[UNK]xMA S,CsqUQxko.Rf,HM:-ZRN'Ty:c\\nVlyGlXNU[UNK]A[UNK]C stjVuLWn'\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**\n",
        "\n",
        "this is use standart classification problem, it will predict the class of the next character.\n",
        "\n",
        "Attach an optimizer and loss function"
      ],
      "metadata": {
        "id": "Ce6q5ht-dDjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#add loss function\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "_q_qGfkDfpgq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \"# (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "print(\"Mean loss:     \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK78QWDOdhGP",
        "outputId": "3aacb7bb-8912-4cc1-d951-4ace7412fe82"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:      tf.Tensor(4.188002, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much higher loss then vocal size mean the model is wrong. The normal one is equal to vocab size."
      ],
      "metadata": {
        "id": "NynTsvWreQ5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LLwwjn7eC3N",
        "outputId": "06572713-fe95-4a09-eee5-44a791a0e85a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.891014"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configyre the training procedure."
      ],
      "metadata": {
        "id": "uAri-Ksfen9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "metadata": {
        "id": "5vi6wFhiefb9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure checkpoints**"
      ],
      "metadata": {
        "id": "0L3P4deke2rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#directory where checkpoints saved\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "#name of checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "8AXFVGzDeyNx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execute training**\n",
        "\n",
        "Use 10 epoch to keep training time reasonable."
      ],
      "metadata": {
        "id": "uEcAaKd9ftqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "EWpkM_AbfYyI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTuUdylsf86T",
        "outputId": "d8cf7937-bfc0-4e97-c022-a583b9909445"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m911s\u001b[0m 5s/step - loss: 3.1489\n",
            "Epoch 2/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m912s\u001b[0m 5s/step - loss: 1.9282\n",
            "Epoch 3/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m900s\u001b[0m 5s/step - loss: 1.6346\n",
            "Epoch 4/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m937s\u001b[0m 5s/step - loss: 1.4882\n",
            "Epoch 5/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m904s\u001b[0m 5s/step - loss: 1.3986\n",
            "Epoch 6/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m920s\u001b[0m 5s/step - loss: 1.3311\n",
            "Epoch 7/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m916s\u001b[0m 5s/step - loss: 1.2808\n",
            "Epoch 8/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 5s/step - loss: 1.2414\n",
            "Epoch 9/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m918s\u001b[0m 5s/step - loss: 1.1997\n",
            "Epoch 10/10\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m908s\u001b[0m 5s/step - loss: 1.1565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate text**\n",
        "\n",
        "simple way to generate text with this model is run by loop."
      ],
      "metadata": {
        "id": "sqJitFpuItOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        #create mask to prevent \"[UNK]\" from being generated\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            #put -inf at each bad index\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            #match the shape to vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())]\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        #convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars)\n",
        "        #convert RaggedTensors to dense tensor\n",
        "        input_ids = input_ids.to_tensor()\n",
        "\n",
        "        #run the model\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        #only use last prediction\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        #apply the prediction mask: prevent \"[UNK]\" from being generated\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        #sample output logits to generate token IDs\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        #convert from token IDs to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        #return the characters and model state\n",
        "        return predicted_chars, states"
      ],
      "metadata": {
        "id": "YMijWnd_gH90"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run it in a loop to generate long text. But, with the small number of training epochs, it has not yet learned to form coherent sentences."
      ],
      "metadata": {
        "id": "ytFnF7gpMs3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "RTe2YU6ZMDgb"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"CHILDER:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd35NUrtMORt",
        "outputId": "b3aa814b-cd9c-401c-b4f5-c9c77d1d80fd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHILDER:\n",
            "What man, I'll try you here?\n",
            "\n",
            "SICINIUS:\n",
            "May be done!\n",
            "I would thee to my crown here to thee,\n",
            "Upon my capers was for prop'd bright.\n",
            "\n",
            "Servant:\n",
            "He ready here let him up.\n",
            "How upreyench out of whom women here?\n",
            "I'll have a king King Herely to my soul.\n",
            "I'll go with mine honourably can garm:\n",
            "I'll have heaven tine; wine reward.\n",
            "What! do, sir, the stage gall'd ones.\n",
            "\n",
            "WARWICK:\n",
            "So us to clut the duke to baid on him;\n",
            "To dress the doom of other\n",
            "Upon my knees, whrether was my skndels wights\n",
            "and post mare much reyour ann tears;\n",
            "Gozner that I loved thee spench. I stand\n",
            "Attwnich heir to us. 'TY O, Caius Marcius,\n",
            "Let's had not brook and lent; it shall near's blood,\n",
            "The villain roward once, to find so please you both.\n",
            "Fie it is much sleep; but ever thou draw now,\n",
            "And were it the bodomity, age tos\n",
            "If Thomas to my friend By once may\n",
            "Unless to my difference; and have you\n",
            "Out, as I tell thee quickly, in their heels,\n",
            "Or let get him each confess to to and\n",
            "But as not smiled table and go\n",
            "and joins, here visit tha \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.5319509506225586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the result use EPOCH=30, bare minimum.\n",
        "\n",
        "Or by adding another RNN layer to improve model accuracy."
      ],
      "metadata": {
        "id": "Fc7NspaGPOiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prEhq3MaNmBX",
        "outputId": "a80c3790-e4bb-47a4-efb6-fbbaf5b3e63a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nShould have I lay that word 'tis like your city\\nWithin my loving son of these fisless faith,\\nOn the heavy dupared, breating, and my\\nneither now you deniced, and undergreet\\nOne post to her here fast to looked\\nBut mender out a man levers together,\\nHow is it to claf our misses and be sobered,\\nPluckle forgot his banies' masquence\\nFrom Romeo blows, blest awhile to talk on death\\nTo free mechers so, and mustering vile,\\nIf they have never but blood this murderer,\\nThe writt-net days for governgury.\\nI should mine honesty in our cousin.\\nWelcome, my lord, great Henry three death!\\nSome mighty neither corsels for her,\\nthe truth of resign whose unpossibler\\nOf the duke of Lord of Clifford\\nWhom valiant best of your blood and Duke of Norfolk,\\nThe our speed as he should come to jest.\\n\\nDUKE VINCENTIO:\\nHow! Which else? They cameftake tom great servant,\\nAs in a miserable, he will to\\ndo it straight, nor he shall not but a postedier.\\n\\nBIANCA:\\nOn Paris lips, Vilig in Veron.\\nHere is God's sake, without chance \"\n",
            " b\"ROMEO:\\nThus home!\\n\\nSAMPoAN:\\nWhen we was cownacet, there, will thy compand\\nSo have I call'd at home, why heaved me\\nwith bloody in my cheatsory both on his honour\\nWith heart this court woe had might have drawn\\nAnd undended body,--\\nWill you perfect thus dange on worth,\\nTo partly proud to us at once.\\n\\nCAMILLO:\\nYou have, but guest, what's the duke?\\n\\nCURTIS:\\nBe mad, this sword, where is the wealth of all\\nstore; wise, nothing, give he leadsheary; both print\\nTo clare consurs but breathed in thy love,\\nunjuckly: where he dreams, with shopes of who?\\n\\nVINCENTIO:\\nI dwell you good with him, and he shall not from me\\nunderstone in that. But dead, being one; none,\\nThese houses once more mildnessy on'e;'\\nAnd beyond the joyful Dernight of fool.\\n\\nLUCENTIO:\\nThus praises no more.\\nWhere is hust undones, making her power inform'd\\nWithout thee this: God save you ware work,\\nI trust my counsel to the bowels of blood\\nWhich have ever farewell of him; but for the better.\\n\\nSom:\\nWhose soundil's filling hands he kill this c\"\n",
            " b\"ROMEO:\\nEntertain a place,\\nUnless the case of my bofing woe.\\n\\nDUKE VINCENTIO:\\nYour humble pastards if the blocks of the house;\\nBut she you can cook upon him, she will renate.\\n\\nShopach,\\nWhy have you indeed, look your uncle, if my fee,\\nThe mortalter husband for sun: and if about thee\\nTo unant Edward's choice, yet use it touch\\nTheir son,'t nay so girld; both use.\\n\\nSICINIUS:\\nSweet gracious sovereign,\\nIf she should living looks.\\nSignior Baptista, mother, I do not, I\\nfrom nothing.\\n\\nDUKE VINCENTIO:\\nFor Gloucester,\\nwise to keep you to the consul, he fair health,\\nTo noge his humble rid of what\\nme in him in: her smallian this I'll take\\neither trivalarity; lockle instruction.\\n\\nQUEEN:\\nWe took sweet sevise,\\nUpon the blind book, the warling elor:\\nIn both one sooner here in the lip of him:\\nFor here is into my rest, that retired\\nMake forth the monard of mine with honours.\\nShould not piss, woildst his business.\\n\\nPRONPES:\\nThen.\\n\\nMONTAGUE:\\nBelieve men's sake, boy: and what they were comfort, nor\\nTo help me palc\"\n",
            " b\"ROMEO:\\nThe nupty asure the clouds and counsel\\nIn this gentle of this hour where;\\nAnd even he will have her face and strength,\\nto ever been no brother, him bright spreed us,\\nAccursed, when whom from base upon him and the flower of his admen\\nthan forly on my grof, by your tongue castle;\\nI am knotent should fresh honours in his vice,\\nThat she proceeds to visit her eyes, speed\\nTo plant for such a place: 'tis trust he will do.\\n\\nCOMINIUS:\\nI'll believe you not poose me to to-night;\\nMost apparent clock, whom I did send not\\nso creature and your divine, mortal-bodits,\\nWhich should he part, my counsellonks, because\\nto their rights, Taking here, will you heve fearful recor\\nDoth chence it been at once. Fare your bed?\\nIf now do sooner? if you, Cankil so.\\n\\nVOLUMNIA:\\nMost all, where's o's other; if he day,\\nTheir professors with and green will restrained\\nTo loubled villain, violently, come,\\nTo sabrig the fill their honey's foot.\\n\\nCAPULET:\\nWhen we shall peece make descant of his friendly.\\nThe very welcome swo\"\n",
            " b\"ROMEO:\\nWe are come, the one have told his untogaid.\\n\\nYORK:\\nCould yeur not of my tender bloody yout,\\nWho comest to not buy his new was not hund there;\\nThe whom we may for the meeting of this penity:\\nDraw king, you as happy work, though it be\\nDoubt in Richmond.'\\n\\nPedant:\\nSo yours, dear-ship.\\n\\nServant:\\nShe colluls unthanked wit many one wounders?\\nMake him my good as honour dividest thou\\nwithstom for very counsel was no further. What say'st thou.\\nHaving now, knave, ere no lewdur on an\\nenemies; but to the sounder taken\\nWhere you make a fool deserved nothing,\\nor worthy thanks; all there will some fivle thither\\nStanding his biriolderse here in destiny,\\nScaroping hery Coriplines.\\n\\nWIRTMENENBR:\\nHow far forbear\\nUpon my dagger blood to find him from the gower of\\nthese brothers to whose good tuss: both you now\\npuredless dend, were not over-thill; my all hers\\nDoth much my free extremity on hers, being spare\\nuseened, and mine and.\\n\\nPERDITA:\\nO, but I'll walk John. I would weep. Come,\\nWhen farewells o' the \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.214131593704224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export the model**"
      ],
      "metadata": {
        "id": "mVKXgcHFPnZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, \"one_step\")\n",
        "one_step_reloaded = tf.saved_model.load(\"one_step\")"
      ],
      "metadata": {
        "id": "SslKdekDPh0e"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant([\"WARWICK:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao9CUUHYP2Y7",
        "outputId": "15cdc147-3095-436b-8b91-d34dfdc09462"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARWICK:\n",
            "Say what 'hose is the caon? I never sad\n",
            "for me come from his babe: but thou didst small have\n",
            "man to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nl0aHyWWQxqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}